# Program
program_seed=0
program_auto_seed=false
program_quiet=false

# Actor
actor_num_simulation=50
actor_mcts_puct_base=19652
actor_mcts_puct_init=1.25
actor_mcts_reward_discount=1
actor_mcts_value_rescale=false
actor_mcts_think_batch_size=1
actor_mcts_think_time_limit=0 # MCTS time limit (in seconds), 0 represents searching without using the time limit
actor_select_action_by_count=false
actor_select_action_by_softmax_count=true
actor_select_action_softmax_temperature=1
actor_select_action_softmax_temperature_decay=false # decay temperature based on zero_end_iteration; use 1, 0.5, and 0.25 for 0%-50%, 50%-75%, and 75%-100% of total iterations, respectively
actor_use_random_rotation_features=true # randomly rotate input features, currently only supports alphazero mode
actor_use_dirichlet_noise=true
actor_dirichlet_noise_alpha=0.03 # 1 / sqrt(num of actions)
actor_dirichlet_noise_epsilon=0.25
actor_use_gumbel=false
actor_use_gumbel_noise=false
actor_gumbel_sample_size=16
actor_gumbel_sigma_visit_c=50
actor_gumbel_sigma_scale_c=1
actor_resign_threshold=-0.9

# Zero
zero_num_threads=4
zero_num_parallel_games=32
zero_server_port=9999
zero_training_directory=
zero_num_games_per_iteration=5000
zero_start_iteration=0
zero_end_iteration=100
zero_replay_buffer=20
zero_disable_resign_ratio=0.1
zero_actor_intermediate_sequence_length=0 # board games: 0; atari: 200
zero_actor_ignored_command=reset_actors # format: command1 command2 ...
zero_actor_stop_after_enough_games=false
zero_server_accept_different_model_games=true

# Learner
learner_use_per=false # Prioritized Experience Replay
learner_per_alpha=1 # Prioritized Experience Replay
learner_per_init_beta=1 # Prioritized Experience Replay
learner_per_beta_anneal=true # linearly anneal PER init beta to 1 based on zero_end_iteration
learner_training_step=500
learner_training_display_step=100
learner_batch_size=1024
learner_muzero_unrolling_step=5
learner_n_step_return=0 # board games: 0, atari: 10
learner_learning_rate=0.01
learner_momentum=0.9
learner_weight_decay=0.0001
learner_value_loss_scale=1
learner_num_thread=8

# Network
nn_file_name=
nn_num_blocks=1
nn_num_hidden_channels=256
nn_num_value_hidden_channels=256
nn_discrete_value_size=1 # set to 1 for the games which doesn't use discrete value
nn_type_name=alphazero # alphazero/muzero

# Environment
env_board_size=19
env_go_komi=7.5
env_go_ko_rule=positional # positional/situational

# Strength Detection
players_per_batch=20
games_per_player=9
n_frames=10
move_step_to_choose=4

